{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752f0909",
   "metadata": {},
   "source": [
    "## 1. Automated Metadata Extraction and Evaluation from Legal Rental Documents and Images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1e3fb",
   "metadata": {},
   "source": [
    "### AIM:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9139e2",
   "metadata": {},
   "source": [
    "To develop a system that automatically extracts key `metadata` fields (like rent value, agreement dates, parties involved, etc.) from rental agreements in `.docx` and `.png` formats using `Optical Character Recognition (OCR)` and `Question-Answering (QA)` models, and evaluates its accuracy against ground truth using `recall`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b823e7",
   "metadata": {},
   "source": [
    "### DATASET:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd38a5",
   "metadata": {},
   "source": [
    "1. **File Name**: The name of the rental agreement file, identifying the document being processed.\n",
    "2. **Aggrement Value**: The monthly rent amount in rupees as an integer, representing the cost of the rental.\n",
    "3. **Aggrement Start Date**: The date the agreement begins, marking the start of the rental period.\n",
    "4. **Aggrement End Date**: The date the agreement ends, indicating the end of the rental period.\n",
    "5. **Renewal Notice (Days)**: The notice period in days required for renewal or termination, specifying how much advance notice is needed.\n",
    "6. **Party One**: The name of the owner or lessor of the agreement, identifying the property owner.\n",
    "7. **Party Two**: The name of the tenant or lessee of the agreement, identifying the resident renting the property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f6dc3",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "1. **Text Extraction**: Extract text from `.docx` files using `python-docx` and from `.png` files using Tesseract OCR.\n",
    "2. **Metadata Extraction**: Use a BERT-based QA model to extract metadata by answering predefined questions.\n",
    "3. **Post-Processing**: Normalize extracted values (e.g., convert text to numbers, format dates).\n",
    "4. **Evaluation**: Compute per-field recall scores by comparing predictions with ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f221c07",
   "metadata": {},
   "source": [
    "#### 1. Code - Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869e7656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda\\envs\\haystack_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import docx\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "import os\n",
    "from word2number import w2n\n",
    "import re\n",
    "\n",
    "# Configure Tesseract path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad26a3c",
   "metadata": {},
   "source": [
    "#### 2. Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25afaa0a",
   "metadata": {},
   "source": [
    "##### 2a. Text Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7045dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from .docx files\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        full_text = [para.text for para in doc.paragraphs if para.text.strip()]\n",
    "        return \"\\n\".join(full_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading .docx file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract text from .png files using OCR\n",
    "def extract_text_from_png(file_path):\n",
    "    try:\n",
    "        image = Image.open(file_path).convert('L')\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.replace('OOO', '000').replace('â€˜', \"'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading .png file {file_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fbb549",
   "metadata": {},
   "source": [
    "##### 2b. Initialize QA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0694194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the QA model (BERT for Question Answering)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4895ba0a",
   "metadata": {},
   "source": [
    "##### 2c. Define Metadata Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4324a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define questions for each metadata field\n",
    "questions = {\n",
    "    \"Aggrement Value\": \"What is the monthly rent amount in rupees?\",\n",
    "    \"Aggrement Start Date\": \"When does the agreement start?\",\n",
    "    \"Aggrement End Date\": \"When does the agreement end?\",\n",
    "    \"Renewal Notice (Days)\": \"How many days in advance should notice be given to renew or end the agreement?\",\n",
    "    \"Party One\": \"Who is the owner of the agreement?\",\n",
    "    \"Party Two\": \"Who is the resident in the agreement?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c35df0",
   "metadata": {},
   "source": [
    "##### 2d. Metadata Extraction Function\n",
    "Extract metadata using the QA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20305808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata using the QA model\n",
    "def extract_metadata(text, questions):\n",
    "    metadata = {}\n",
    "    for field, question in questions.items():\n",
    "        result = qa_pipeline(question=question, context=text)\n",
    "        metadata[field] = result['answer']\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d597f",
   "metadata": {},
   "source": [
    "#### 3. Process Test Files and Save Predictions\n",
    "Process all files in the test folder, extract metadata, and save to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09cc1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files in the test folder\n",
    "def process_test_files(test_folder, output_csv):\n",
    "    test_files = os.listdir(test_folder)\n",
    "    predictions = []\n",
    "\n",
    "    for file_name in test_files:\n",
    "        file_path = os.path.join(test_folder, file_name)\n",
    "        text = extract_text_from_docx(file_path) if file_name.endswith('.docx') else extract_text_from_png(file_path)\n",
    "        metadata = extract_metadata(text, questions)\n",
    "        metadata['File Name'] = file_name.split('.')[0]\n",
    "\n",
    "        # Value\n",
    "        try:\n",
    "            metadata['Aggrement Value'] = w2n.word_to_num(metadata['Aggrement Value'])\n",
    "        except:\n",
    "            metadata['Aggrement Value'] = int(re.sub(r'[^0-9]', '', metadata['Aggrement Value']))\n",
    "\n",
    "        # Notice Period\n",
    "        input_text = metadata[\"Renewal Notice (Days)\"].strip().lower()\n",
    "        days = 0\n",
    "        if match := re.match(r'(\\d+|\\w+)\\s*(month|months)', input_text):\n",
    "            try: days = int(match.group(1)) * 30\n",
    "            except: days = w2n.word_to_num(match.group(1)) * 30\n",
    "        elif match := re.match(r'(\\d+|\\w+)\\s*day', input_text):\n",
    "            try: days = int(match.group(1))\n",
    "            except: days = w2n.word_to_num(match.group(1))\n",
    "        metadata[\"Renewal Notice (Days)\"] = days\n",
    "\n",
    "        # Dates\n",
    "        for key in ['Aggrement Start Date', 'Aggrement End Date']:\n",
    "            date_str = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', metadata[key])\n",
    "            date_str = date_str.replace(' of', '').replace('*', '').replace(',', '').strip()\n",
    "            try:\n",
    "                parsed_date = datetime.strptime(date_str, '%d %B %Y')\n",
    "                metadata[key] = parsed_date.strftime('%d.%m.%Y')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        predictions.append(metadata)\n",
    "    # Save predictions to CSV\n",
    "    df = pd.DataFrame(predictions)\n",
    "    df = df[['File Name', 'Aggrement Value', 'Aggrement Start Date', 'Aggrement End Date', 'Renewal Notice (Days)', 'Party One', 'Party Two']]\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ed102",
   "metadata": {},
   "source": [
    "#### 4. Compute Recall\n",
    "Evaluate the predictions by computing per-field recall against ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c6f6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-field Recall\n",
    "def compute_recall(pred_df, gt_df):\n",
    "    scores = {}\n",
    "    for col in ['Aggrement Value', 'Aggrement Start Date', 'Aggrement End Date', 'Renewal Notice (Days)', 'Party One', 'Party Two']:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for _, row in pred_df.iterrows():\n",
    "            gt_row = gt_df[gt_df['File Name'] == row['File Name']]\n",
    "            if gt_row.empty: continue\n",
    "            gt_val = gt_row.iloc[0][col]\n",
    "            pred_val = row[col]\n",
    "            # Handle potential missing values\n",
    "            if pd.isna(gt_val) or pd.isna(pred_val): continue\n",
    "            # Compare values\n",
    "            if col in ['Aggrement Value', 'Renewal Notice (Days)']:\n",
    "                if gt_val == pred_val:\n",
    "                    correct += 1\n",
    "            elif col in ['Aggrement Start Date', 'Aggrement End Date']:\n",
    "                try:\n",
    "                    if pd.to_datetime(gt_val, dayfirst=True) == pd.to_datetime(pred_val, dayfirst=True):\n",
    "                        correct += 1\n",
    "                except: continue\n",
    "            else:\n",
    "                # For Party One and Party Two, compare strings (case-insensitive)\n",
    "                if str(gt_val).strip().lower() == str(pred_val).strip().lower():\n",
    "                    correct += 1\n",
    "            total += 1\n",
    "        scores[col] = correct / total if total > 0 else 0\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69d98a",
   "metadata": {},
   "source": [
    "#### 5. Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0f6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = \"data/test\"\n",
    "test_csv = \"data/test.csv\"\n",
    "output_csv = \"predictions.csv\"\n",
    "\n",
    "# Load ground truth\n",
    "gt_df = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca0211",
   "metadata": {},
   "source": [
    "#### 6. Run the pipeline and compute recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae17c593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-field Recall Scores:\n",
      "Aggrement Value: 1.00\n",
      "Aggrement Start Date: 1.00\n",
      "Aggrement End Date: 0.50\n",
      "Renewal Notice (Days): 0.75\n",
      "Party One: 0.25\n",
      "Party Two: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Process test files and get predictions\n",
    "pred_df = process_test_files(test_folder, output_csv)\n",
    "\n",
    "# Compute Recall scores\n",
    "recall_scores = compute_recall(pred_df, gt_df)\n",
    "\n",
    "# Print Recall scores\n",
    "print(\"Per-field Recall Scores:\")\n",
    "for field, score in recall_scores.items():\n",
    "    print(f\"{field}: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
